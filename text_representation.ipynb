{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "HMVmY8T2lvCl",
        "outputId": "e58779b8-366c-4a90-a283-b577f3f26f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.preprocessing.text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-227e9320d919>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.preprocessing.text'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# prompt: from keras.preprocessing.text import tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from keras.preprocessing.text import tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "Bb-XWv3BmLcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    'the cat sat',\n",
        "    'the cat sat in the hat',\n",
        "    'the cat with the hat',\n",
        "]"
      ],
      "metadata": {
        "id": "T2cFbe-fm9gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(docs)\n",
        "print(f'vocabulary: {list(tokenizer.word_index.keys())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew1IJyNbnQTp",
        "outputId": "90984456-57a6-4a58-fca2-35953881f5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary: ['the', 'cat', 'sat', 'hat', 'in', 'with']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count bow\n",
        "vectors = tokenizer.texts_to_matrix(docs, mode='count')\n",
        "print(vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJNp7ruxn388",
        "outputId": "224f784b-48f5-45fe-fc0f-19fe97bd4e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 1. 1. 0. 0. 0.]\n",
            " [0. 2. 1. 1. 1. 1. 0.]\n",
            " [0. 2. 1. 0. 1. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "F1ClzT_ZoZ8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data\n",
        "texts = [\n",
        "    \"I love programming\",\n",
        "    \"Programming is fun\",\n",
        "    \"I love deep learning\",\n",
        "]\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10)  # Keep only the top 10 words (you can adjust this)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Convert sequences to one-hot encoded binary matrix\n",
        "one_hot_results = tokenizer.texts_to_matrix(texts, mode='binary')"
      ],
      "metadata": {
        "id": "u2iqsbKYoyOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "print(\"Word Index:\", tokenizer.word_index)\n",
        "print(\"\\nSequences:\", sequences)\n",
        "print(\"\\nOne-Hot Encoded Matrix:\\n\", one_hot_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY_cTVNhpCfQ",
        "outputId": "c143041b-1a21-4bd0-eeaf-f1313c8d6289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index: {'i': 1, 'love': 2, 'programming': 3, 'is': 4, 'fun': 5, 'deep': 6, 'learning': 7}\n",
            "\n",
            "Sequences: [[1, 2, 3], [3, 4, 5], [1, 2, 6, 7]]\n",
            "\n",
            "One-Hot Encoded Matrix:\n",
            " [[0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 0. 0. 0. 1. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf based encoding\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"I love programming\",\n",
        "    \"Programming is fun\",\n",
        "    \"I love deep learning\",\n",
        "]\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the text data and transform the text to TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Convert the TF-IDF matrix to a dense format (optional, for better visualization)\n",
        "dense_tfidf_matrix = tfidf_matrix.toarray()\n",
        "\n",
        "# Display the vocabulary and the TF-IDF matrix\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.vocabulary_)\n",
        "print(\"\\nTF-IDF Matrix:\\n\", dense_tfidf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlVBnEjwqh2-",
        "outputId": "895c7fa0-0957-473b-f28e-f5da5e3f50a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'love': 4, 'programming': 5, 'is': 2, 'fun': 1, 'deep': 0, 'learning': 3}\n",
            "\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.         0.70710678 0.70710678]\n",
            " [0.         0.62276601 0.62276601 0.         0.         0.4736296 ]\n",
            " [0.62276601 0.         0.         0.62276601 0.4736296  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# distributional semantics word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "IkRhFcFfqt9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data\n",
        "texts = [\n",
        "    \"I love programming\",\n",
        "    \"Programming is fun\",\n",
        "    \"I love deep learning\",\n",
        "    \"Deep learning is a branch of machine learning\",\n",
        "]"
      ],
      "metadata": {
        "id": "s4_hW1-drYcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text data\n",
        "processed_texts = [simple_preprocess(text) for text in texts]"
      ],
      "metadata": {
        "id": "93Q0FUXUr13c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Word2Vec model\n",
        "model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "01US0CD_r6Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector for a specific word\n",
        "word_vector = model.wv['programming']"
      ],
      "metadata": {
        "id": "Bl6TBzx4sQ8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the most similar words to 'programming'\n",
        "similar_words = model.wv.most_similar('programming')"
      ],
      "metadata": {
        "id": "u8W23SnysYLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p40ztklRsfi0",
        "outputId": "7c49f909-587b-45db-be2e-cd075180b2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('machine', 0.13149002194404602),\n",
              " ('fun', 0.07497558742761612),\n",
              " ('deep', 0.06804502755403519),\n",
              " ('of', 0.04157734289765358),\n",
              " ('is', -0.013514931313693523),\n",
              " ('branch', -0.013679763302206993),\n",
              " ('love', -0.044617101550102234),\n",
              " ('learning', -0.1116705983877182)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the word vector and similar words\n",
        "print(\"Vector for 'programming':\\n\", word_vector)\n",
        "print(\"\\nWords most similar to 'programming':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D-zyWWZsql7",
        "outputId": "c8aa1494-32a5-44bc-c038-d1afa70960fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'programming':\n",
            " [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
            "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
            "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
            "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
            "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
            "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
            " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
            "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
            " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
            "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
            "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
            "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
            " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
            " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
            "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
            "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
            "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
            " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
            " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
            " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
            " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
            " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
            " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
            " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
            "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n",
            "\n",
            "Words most similar to 'programming': [('machine', 0.13149002194404602), ('fun', 0.07497558742761612), ('deep', 0.06804502755403519), ('of', 0.04157734289765358), ('is', -0.013514931313693523), ('branch', -0.013679763302206993), ('love', -0.044617101550102234), ('learning', -0.1116705983877182)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe\n",
        "# Download GloVe vectors (e.g., 100D vectors)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPb61znHsr9G",
        "outputId": "8499c1e6-88a1-4bc7-8245-7d856a311501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-27 16:58:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-08-27 16:58:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-08-27 16:58:09--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip         36%[======>             ] 304.08M  5.08MB/s    eta 95s    ^C\n",
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"
          ]
        }
      ]
    }
  ]
}